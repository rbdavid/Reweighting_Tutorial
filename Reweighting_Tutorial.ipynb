{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reweighting Tutorial\n",
    "So, you've done some enhanced sampling simulations where you applied a biasing potential to a certain collective variable. For whatever reason, you now want to quantify the free energy surface of the modeled event in a collective variable that is _not the biased CV_. Having applied an artificial bias to the simulations, you cannot simply calculate the probability density of this new CV and convert to free energy. Instead, each frame's weight in the probability density must be measured based on the weight that the frame received in the biased CV space. \n",
    "\n",
    "## Analysis Steps:\n",
    "### 1) Run EMUS or WHAM to get stitching constants in the biased CV FE surfaces\n",
    "Before any analysis of the new CV can be done, we need to perform a free energy analysis (using either EMUS or WHAM; suggested to use EMUS) of the biased CV space to obtain the \"stitching\" or \"normalization\" constant for each window in the baised CV space. This constant is important when reweighting the biased CV sampling into a new collective variable space. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "from emus import emus,avar,usutils\n",
    "import IO\n",
    "\n",
    "emus_meta_file = sys.argv[1]\n",
    "nWindows = int(sys.argv[2])\n",
    "start = int(sys.argv[3])\n",
    "end = int(sys.argv[4])\n",
    "projected_data_files_naming = sys.argv[5]\n",
    "cv_data_files_naming = sys.argv[6]\n",
    "xmin = float(sys.argv[7])\n",
    "xmax = float(sys.argv[8])\n",
    "xbins = int(sys.argv[9])\n",
    "nIterations = int(sys.argv[10])\n",
    "x_axis_label = sys.argv[11]\n",
    "\n",
    "T = 300. #units of Kelvin\n",
    "k_B = 0.0019872036 # kcal K^-1 mol^-1\n",
    "beta = 2*k_B*T\n",
    "\n",
    "print('Starting EMUS analysis')\n",
    "\n",
    "psis, cv_trajs, neighbors = usutils.data_from_meta(emus_meta_file,1,T=T,k_B=k_B) \n",
    "# psis is a list of arrays containing the relative weights of each window's cv values in all windows. \n",
    "# cv_trajs is a list of arrays containing the raw cv values for each window. \n",
    "# neighbors is an 2D array containing indices for neighboring windows to be used. \n",
    "\n",
    "# calculate the partition function for each window\n",
    "z, F = emus.calculate_zs(psis,neighbors=neighbors,n_iter=1000)  \n",
    "# z is an 1D array that contains the \"stitching\" or normalization constant for each window. \n",
    "# F is an 2D array (num windows by num windows) that contains the eigenvalues of the iterature EMUS process.\n",
    "\n",
    "zerr, zcontribs, ztaus = avar.calc_partition_functions(psis,z,F,iat_method='acor')\n",
    "# zerr is an 1D array that contains the error values for the z values.\n",
    "# zcontribs ...\n",
    "# ztaus ...\n",
    "\n",
    "np.savetxt('emus_stitching_constants.dat',np.c_[list(range(int(parameters['nWindows']))),z,zerr], fmt='%15.10f')\n",
    "\n",
    "r0_k_data, file_list = IO.read_meta(parameters['emus_meta_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Histogram new, unbiased collective variable data, adding the reweighted counts to the respective histogram bin to calculate the probability density/free energy. \n",
    "Each frame's weight is added to a the respective bin associated with the new, unbiased CV value. The weight of a frame is calculated using the frame's biased CV value, the stitching constants (calculated from the last step), and the equilibrium constants used for each window of the biased simulations. The corrected weight of a frame from a biased simulation is the inverse of the sum of volume-corrected (if needed) Boltzmann weights from each biasing potential used in the set of analyzed simulations. \n",
    "\n",
    "$$ w(t) = \\frac{1}{nValues * BinWidth * VolumeCorrection(t) * \\frac{1}{nWindows} \\sum \\limits_{i}^{nWindows} \\frac{1}{Z_{i}} exp\\left[-\\frac{k_{i}}{2k_{B}T}(r(t) - r_{i})^{2}\\right]} $$\n",
    "\n",
    "where $w(t)$ is the weight of a frame, t, $nValues$ is the total number of frames being analyzed, $BinWidth$ is the width of histogram bins used for the new CV histogram, $VolumeCorrection(t)$ is a term included to normalize the volume of the biased CV space and is a frame-dependent value, $nWindows$ is the number of windows being summed over, and the exponential within the sum is the Boltzmann weight of frame t's biased CV value ($r(t)$ relative to window i's equilibrium value, $r_{i}$, and biasing force constant, $k_{i}$) scaled by the window i's stitching constant. The $nValues$ and $BinWidth$ terms are used to convert a count into the associated probability density value. A volume correction term is needed when entropy of states is not uniform throughout the biased collective variable space (e.g. more volume at large distances than at close distances). The remaining terms in the denominators represents the average Boltzmann factor associated with the frame's biased CV value within all windows. \n",
    "\n",
    "A frame's weight is added to the respective bin in the new, unbiased CV histogram. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NEED TO LOAD IN BIASED AND UNBIASED CV DATA\n",
    "# Creating the data structure that will be used for the remainder of the analysis...\n",
    "# data will be a 2D array of shape nValues x 2; index 0 will be biased CV, index 1 will be unbiased CV\n",
    "print('Starting to load in cv data (original and new)')\n",
    "nWindows_range = range(nWindows) # assumes windows are numbered with zero-indexing\n",
    "nProductions_range = range(start,end+1)\n",
    "data = ['' for i in nWindows_range]\n",
    "for i in nWindows_range:\n",
    "    print('loading window', i)\n",
    "    temp_projected_data = np.loadtxt(projected_data_files_naming%(i,i))[:,:2]     # NOTE: assumes a format of data file;\n",
    "    temp_data = np.zeros((len(temp_projected_data),3))\n",
    "    temp_data[:,1] = temp_projected_data[:,0]   # unbiased CV1 data is index 1\n",
    "    count = 0\n",
    "    for j in nProductions_range:\n",
    "        temp_cv_data = np.loadtxt(cv_data_files_naming%(j,i,j),skiprows=1) # NOTE: assumes a format of data file;\n",
    "        temp_data[count:count+len(temp_cv_data),0] = temp_cv_data[:,1]  # biased CV data is index 0\n",
    "        count += len(temp_cv_data)\n",
    "    data[i] = temp_data\n",
    "    if count != len(temp_projected_data):\n",
    "        print('unbiased data file', i, 'has different number of lines than the combined cv data from the respective window. This should not happen.', count, len(temp_projected_data))\n",
    "        sys.exit()\n",
    "\n",
    "delta_x = (xMax - xMin)/xBins\n",
    "x_half_bins = np.array([xMin + delta_x*(i+0.5) for i in range(xBins)])\n",
    "x_edges = np.array([xMin + delta_x*i for i in range(xBins+1)])\n",
    "\n",
    "nValues_total = 0\n",
    "x_total_fe_counts = np.zeros(xBins)\n",
    "for i in nWindows_range:\n",
    "    nValues = len(data[i])\n",
    "    nValues_total += nValues\n",
    "    \n",
    "    x_window_counts = np.zeros(xBins)\n",
    "    x_window_fe_counts = np.zeros(xBins)\n",
    "\n",
    "    for j in range(nValues):\n",
    "        # ----------------------------------------\n",
    "        # HISTOGRAMMING UNBIASED CV DATA POINT\n",
    "        x_index = int((data[i][j][1] - xMin)/delta_x)\n",
    "        if x_index < 0 or x_index > xBins:\n",
    "            print('!!! 0 > x_index >= xBins ...', data[i][j][0], x_index, i, 'Histogram bounds are not wide enough in the x-dimension. Job failed.')\n",
    "            sys.exit()\n",
    "        elif x_index == xBins:\n",
    "            x_index = -1\n",
    "    \n",
    "        # ----------------------------------------\n",
    "        # CALCULATING WEIGHT OF BIASED DATA POINT IN CONSIDERATION OF CURRENT WINDOW ONLY (USEFUL FOR PLOTTING PURPOSES)\n",
    "        w = np.exp((-beta*r0_k_data[i][1])*(data[i][j][0] - r0_k_data[i][0])**2)/z[i]     # exp((-k/2*k_B*T)(r-r0)^2)/z; no volume correction...\n",
    "        #w = (data[i][j][0]**2)*np.exp((-beta*r0_k_data[i][1])*(data[i][j][0] - r0_k_data[i][0])**2)/z[i]     # r^2 * exp((-k/2*k_B*T)(r-r0)^2)/z;; volume correction for distance space\n",
    "        x_window_counts[x_index] += 1\n",
    "        x_window_fe_counts[x_index] += 1/w\n",
    "    \n",
    "        # ----------------------------------------\n",
    "        # CALCULATING WEIGHT OF BIASED DATA POINT IN CONSIDERATION OF ALL WINDOWS\n",
    "        w = 0\n",
    "        for k in nWindows_range:\n",
    "            w+= np.exp((-beta*r0_k_data[k][1])*(data[i][j][0] - r0_k_data[k][0])**2)/z[k]       # exp((-k/2*k_B*T)(r-r0)^2)/z; no volume correction...\n",
    "            #w+= (data[i][j][0]**2)*np.exp((-beta*r0_k_data[k][1])*(data[i][j][0] - r0_k_data[k][0])**2)/z[k]       # r^2 * exp((-k/2*k_B*T)(r-r0)^2)/z; \n",
    "        w /= nWindows # <r^2 * exp((-k/2*k_B*T)(r-r0)^2)/z>; average reciprocal boltzmann weight of data point in all possible windows;\n",
    "        \n",
    "        x_total_fe_counts[x_index] += 1/w\n",
    "        \n",
    "    # ----------------------------------------\n",
    "    # SEE STEP 4 BELOW: PLOTTING\n",
    "    # FINISHING ANALYSIS OF THE REWEIGHTED PROB. DENSITY OF EACH INDIVIDUAL WINDOW\n",
    "    x_window_prob_density = x_window_counts/(nValues*delta_x)\n",
    "    plt.figure(1)\n",
    "    plt.plot(x_half_bins[:],x_window_prob_density[:],zorder=3)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # FINISHING ANALYSIS OF THE REWEIGHTED FREE ENERGY OF EACH INDIVIDUAL WINDOW\n",
    "    x_window_fe_counts = -kT*np.log(x_window_fe_counts/(nValues*delta_x))  # no volume correction\n",
    "    #x_window_fe_counts = -kT*np.log(x_window_fe_counts/(nValues*delta_x*four_pi))\n",
    "    plt.figure(2)\n",
    "    plt.plot(x_half_bins[x_window_counts > 10.], x_window_fe_counts[x_window_counts > 10],zorder=3)\n",
    "    \n",
    "    print('Done with window', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Bootstrapping to calculate a lower bound for the error associated with the reweighting results. \n",
    "A bootstrapping analysis is used to estimate error for the new reweighted free energy surface. A number of bootstrapping iterations is performed where, during an iteration, $nValues$ of data is randomly pulled from the original collective variable data set. This new dataset is used to calculate the respective, fake free energy surface. The free energy surfaces from all bootstrapping iterations are then used to calculate the standard deviation of the sample data. This error analysis uses random sampling from a dataset with replacement, treating each data point within the original dataset as an independent, uncorrelated data point. Due to this assumption, the standard deviation calculated from bootstrapping represents the lower estimate for error in the reweighted free energy. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Beginning bootstrap analysis to approximate error in reweighted CVs')\n",
    "original_data = np.empty((0,2))\n",
    "for i in nWindows_range:\n",
    "    original_data = np.concatenate((original_data,np.array(data[i])))\n",
    "\n",
    "if original_data.shape != (nValues_total,3):\n",
    "    print(original_data.shape, nValues_total, 'something went wrong during bootstrapping')\n",
    "    sys.exit()\n",
    "\n",
    "x_bootstrap_results = []\n",
    "for i in range(nIterations):\n",
    "    print('Starting Step %d of %d steps in bootstrap analysis'%(i,nIterations))\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # CREATING SAMPLE DATA FOR BOOTSTRAP ITERATION\n",
    "    sample_data = original_data[np.random.randint(nValues_total,size=nValues_total)]\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # HISTOGRAMMING UNBIASED CV DATA POINT\n",
    "    for j in range(nValues_total):\n",
    "        x_index = int((sample_data[j,1] - xMin)/delta_x)\n",
    "        if x_index == xBins:\n",
    "            x_index = -1\n",
    "        \n",
    "        w = 0\n",
    "        for k in nWindows_range:\n",
    "            w+= np.exp((-beta*r0_k_data[k][1])*(sample_data[j,0] - r0_k_data[k][0])**2)/z[k]       # exp((-k/2*k_B*T)(r-r0)^2)/z; no volume correction...\n",
    "        w /= nWindows # <r^2 * exp((-k/2*k_B*T)(r-r0)^2)/z>; average reciprocal boltzmann weight of data point in all possible windows;\n",
    "        x_total_fe_bootstrap[x_index] += 1/w\n",
    "        x_total_fe_bootstrap = -kT*np.log(x_total_fe_bootstrap/(nValues*delta_x))  # no volume correction\n",
    "        x_total_fe_bootstrap -= np.ndarray.min(x_total_fe_bootstrap)\n",
    "        x_bootstrap_results.append(x_total_fe_bootstrap)\n",
    "        \n",
    "### NOTE: CALCS THE STANDARD DEVIATION OF THE BOOTSTRAPPED DATA\n",
    "x_std_error = np.std(np.array(x_bootstrap_results),axis=0)\n",
    "np.savetxt('x_data_bootstrap_error.dat', x_std_error, fmt='%15.10f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Perform plotting during the reweighting analysis for sanity checks. \n",
    "Plotting of probability densities and unstitched free energy graphs are useful to present the quality of sampling and stitching in the new CV space."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ----------------------------------------\n",
    "# FINISHING PLOTTING OF THE REWEIGHTED PROB. DENSITY OF EACH INDIVIDUAL WINDOW - XDATA\n",
    "IO.finish_plot(1,'reweighted_x_axis_prob_density.png',x_axis_label,'Probability Density',xlim=(xMin,xMax))\n",
    "\n",
    "# ----------------------------------------\n",
    "# FINISHING PLOTTING OF THE REWEIGHTED, UNSTITCHED FREE ENERGY - XDATA\n",
    "IO.finish_plot(2,'reweighted_x_axis_unstitched_fe.png',x_axis_label,r'Relative Free Energy (kcal mol$^{-1}$)',xlim=(xMin,xMax))\n",
    "\n",
    "# ----------------------------------------\n",
    "# PLOTTING REWEIGHTED X-DATA FE SURFACE\n",
    "x_total_fe_counts = -kT*np.log(x_total_fe_counts/(delta_x*nValues_total)) # no volume correction\n",
    "np.savetxt('reweighted_x_axis_stitched_fe.dat', np.c_[range(xBins),x_half_bins,x_total_fe_counts], fmt='%15.10f')\n",
    "\n",
    "plt.figure(3)\n",
    "plt.errorbar(x_half_bins[:],x_total_fe_counts[:],yerr=x_std_error,ecolor='r',elinewidth=0.5,zorder=3)\n",
    "IO.finish_plot(3, 'reweighted_x_axis_stitched_fe.png', x_axis_label, r'Relative Free Energy (kcal mol$^{-1}$)',xlim=(xMin,xMax),ylim=(-0.05,10)) # NOTE\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
